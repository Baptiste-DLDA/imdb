{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1d288-8f08-4aa4-a133-9a70fe72832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\kouba\\OneDrive\\Bureau\\IngÃ©2\\Projet_6\\tensorflow_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kouba\\OneDrive\\Bureau\\IngÃ©2\\Projet_6\\tensorflow_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import tensorflow_hub as hub\n",
    "from keras import layers\n",
    "import bert\n",
    "import re\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tensorflow.python.keras.models import save_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6ef4-02ef-42c4-b9a5-9ef315e66d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7f042-6fd7-42f0-9dda-5ad5f9586b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683de85-e74d-4704-9421-355c170b0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE  = re.compile(r'<[^>]+>')\n",
    "def remove_tags(txt):\n",
    "    return TAG_RE.sub('', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806f17d-954f-42d4-a2d2-db8ed9970acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preflitrage_txt(x):\n",
    "    phrase = remove_tags(x)\n",
    "    phrase = re.sub('[^a-zA-Z]',' ', phrase)\n",
    "    phrase = re.sub(r\"\\s+[a-zA-Z]\\s+\",' ',phrase)\n",
    "    phrase = re.sub(r'\\s+', ' ', phrase)\n",
    "    return phrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183c183-3fda-4960-b073-f857e64f6043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review' 'sentiment']\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "phrases = list(df['review'])\n",
    "for x in phrases :\n",
    "    reviews.append(preflitrage_txt(x))\n",
    "print(df.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99704fd-e8fb-44cb-af5b-e66f4e09f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e47ed2-c8c7-44e9-9e57-d9bd0bb1b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines At first it was very odd and pretty funny but as the movie progressed didn find the jokes or oddness funny anymore Its low budget film thats never problem in itself there were some pretty interesting characters but eventually just lost interest imagine this film would appeal to stoner who is currently partaking For something similar but better try Brother from another planet \n"
     ]
    }
   ],
   "source": [
    "y = df['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\n",
    "print(reviews[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d1a09-ebb8-4612-be6e-12ff110204dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['don', \"'\", 't', 'be', 'so', 'judgment', '##al']\n",
      "IDs des tokens : [2123, 1005, 1056, 2022, 2061, 8689, 2389]\n"
     ]
    }
   ],
   "source": [
    "# Chargement du tokenizer directement via Hugging Face\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = tokenizer.tokenize(\"don't be so judgmental\")\n",
    "print(\"Tokens :\", tokens)\n",
    "\n",
    "# Conversion des tokens en IDs\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"IDs des tokens :\", tokens_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a22c6a-9d15-439a-acfc-83a2c4470745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))\n",
    "\n",
    "tokenized_reviews = [tokenize_reviews(review) for review in reviews ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398943c-7568-40d2-b09c-023df952ba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 21), dtype=int32, numpy=\n",
       " array([[ 2054,  5896,  2054,  2466,  2054,  6752,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3191,  1996,  2338,  5293,  1996,  3185,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 3078,  5436,  3078,  3257,  3532,  7613,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2062, 23873,  3993,  2062, 11259,  2172,  2172,  2062, 14888,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  3185,  2003,  6659,  2021,  2009,  2038,  2070,  2204,\n",
       "          3896,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2876,  9278,  2023,  2028,  2130,  2006,  7922, 12635,\n",
       "          2305,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  3246,  2023,  2177,  1997,  2143, 11153,  2196,  2128,\n",
       "         15908,  2015,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 8235,  1998,  3048,  4616,  2011,  3419,  2457, 27727,  1998,\n",
       "          2848, 16133,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7918, 14674,  7662,  2003,  6581,  2003,  2023,  2143,  2002,\n",
       "          3084, 17160,  2450,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2307,  3185,  2205,  2919,  2009,  2003,  2025,\n",
       "          2800,  2006,  2188,  2678,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [11861,  1996, 21442,  6895,  3238,  2515,  2210, 22759,  6198,\n",
       "          1998,  3185,  2087, 12487,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2017,  2488,  5454,  2703,  2310, 25032,  8913,  8159,  2130,\n",
       "          2065,  2017,  2031,  3427,  2009,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2053,  7615,  5236,  3185,  3772,  2779,  2030,  4788,  9000,\n",
       "          2053,  3168,  2012,  2035, 13558,  2009,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 1045,  2123,  2113,  2339,  2066,  2023,  3185,  2061,  2092,\n",
       "          2021,  2196,  2131,  5458,  1997,  3666,  2009,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2074,  2293,  1996,  6970, 13068,  2090,  2048,  2307,  3494,\n",
       "          1997,  2754,  3898,  2310,  3593,  2102,  6287,  5974,     0,\n",
       "             0,     0,     0],\n",
       "        [ 7615,  2023,  3185,  2003,  5263,  2003,  6659,  2200, 17727,\n",
       "          3217,  3676,  3468,  2919,  7613,  3257,  2025,  2298,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2146, 11771,  1038,  8523,  8458,  6633,  3560,  2196,  2031,\n",
       "          2042,  2061,  5580,  2000,  2156,  4566,  6495,  4897,     0,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  2204,  2143,  2023,  2003,  2200,  6057,  2664,\n",
       "          2044,  2023,  2143,  2045,  2020,  2053,  2204,  8471,  3152,\n",
       "             0,     0,     0],\n",
       "        [ 7078, 10392,  3649,  2360,  2876,  2079,  2023,  2104,  9250,\n",
       "          3185,  1996,  3425,  2009, 17210,  3422,  2009,  2085, 10392,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 2023,  2003,  1996, 15764,  3185,  2544,  1997,  8429, 24905,\n",
       "         17988,  7659,  2498,  2021,  2045,  2024,  2053, 13842,  5312,\n",
       "             0,     0,     0],\n",
       "        [ 5587,  2023,  2210, 17070,  2000,  2115,  2862,  1997,  6209,\n",
       "         24945,  2009, 26354, 28394,  2102,  6057,  1998,  2203, 27242,\n",
       "             0,     0,     0],\n",
       "        [ 1037,  7244,  3185,  2009,  2003,  2440,  1997,  6699,  1998,\n",
       "          6919,  3772,  2071,  2031,  2938,  2083,  2009,  2117,  2051,\n",
       "             0,     0,     0],\n",
       "        [ 2235,  3077,  2792,  3425,  2003,  1996,  2190,  2792,  1997,\n",
       "          2235,  3077,  2009,  2026,  5440,  2792,  1997,  2235,  3077,\n",
       "             0,     0,     0],\n",
       "        [ 1037,  2033,  6491, 11124,  6774,  2143,  2008,  5121,  7906,\n",
       "          2115,  3086,  3841, 13196,  2003, 17160,  1998, 26103,  2000,\n",
       "          3422,     0,     0],\n",
       "        [ 6283,  2009,  2007,  2035,  2026,  2108,  5409,  3185,  2412,\n",
       "         10597, 21985,  2393,  2033,  2009,  2001,  2008,  2919,  3404,\n",
       "          2033,     0,     0],\n",
       "        [ 1037,  5790,  1997,  2515,  2025,  4088,  2000,  4671,  2129,\n",
       "         10634,  2139, 24128,  1998, 21660,  2135,  2919,  2023,  3185,\n",
       "          2003,     0,     0],\n",
       "        [ 2005,  5760,  7788,  4393,  8808,  2498,  2064, 12826,  2000,\n",
       "          1996, 11056,  3152,  3811, 16755,  2169,  1998,  2296,  2028,\n",
       "          1997,  2068,     0],\n",
       "        [ 7244,  2092,  2856, 10828,  1997, 10904,  2402,  2472,  3135,\n",
       "          2293,  2466,  2007, 10958,  8428, 10102,  1999,  1996,  4281,\n",
       "          4276,  3773,     0],\n",
       "        [ 2028,  1997,  1996,  4569, 15580,  2102,  5691,  2081,  1999,\n",
       "          3522,  2086,  2204, 23191,  5436,  1998, 11813,  6370,  2191,\n",
       "          2023,  2028,  4438],\n",
       "        [ 2023,  3185,  2097,  2467,  2022,  5934,  1998,  3185,  4438,\n",
       "          2004,  2146,  2004,  2045,  2024,  2145,  2111,  2040,  6170,\n",
       "          3153,  1998,  2552],\n",
       "        [ 2023,  2003,  6659,  3185,  2123,  5949,  2115,  2769,  2006,\n",
       "          2009,  2123,  2130,  3422,  2009,  2005,  2489,  2008,  2035,\n",
       "          2031,  2000,  2360]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0], dtype=int32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_with_len = [[review, y[i], len(review)] \n",
    "                    for i, review in enumerate(tokenized_reviews)]\n",
    "\n",
    "import random\n",
    "random.shuffle(reviews_with_len)\n",
    "\n",
    "reviews_with_len.sort(key=lambda x: x[2])\n",
    "\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\n",
    "\n",
    "def data_generator():\n",
    "    for review, label in sorted_reviews_labels:\n",
    "        yield (review, label)\n",
    "\n",
    "processed_dataset = tf.data.Dataset.from_generator(data_generator, \n",
    "                                                    output_signature=(\n",
    "                                                        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                                                        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "                                                    ))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None,), ()))\n",
    "\n",
    "next(iter(batched_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9034a6-b060-4730-8b3c-b46d37e0f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f874b2-8885-45b8-b56c-2973b1f6603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                vocabulary_size,\n",
    "                embedding_dimensions=128,\n",
    "                cnn_filters=50,\n",
    "                dnn_units=512,\n",
    "                model_output_classes=2,\n",
    "                dropout_rate=0.1,\n",
    "                training=False,\n",
    "                name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "\n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                        embedding_dimensions)\n",
    "\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.last_dense = layers.Dense(units=1,activation=\"sigmoid\")\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l)\n",
    "        l_1 = self.pool(l_1)\n",
    "        l_2 = self.cnn_layer2(l)\n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3)\n",
    "\n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=1)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training=True)\n",
    "        model_output  = self.last_dense(concatenated)\n",
    "        return model_output\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16c73d0-634f-441e-a778-bbe944c5418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00756c25-b92f-4f98-9e2b-2d5a5d0dff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                               embedding_dimensions=EMB_DIM,\n",
    "                               cnn_filters=CNN_FILTERS,\n",
    "                               dnn_units=DNN_UNITS,\n",
    "                               model_output_classes=OUTPUT_CLASSES,\n",
    "                               dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa07a01-5fd6-409a-8819-185a0758be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.compile(loss=\"binary_crossentropy\",\n",
    "                   optimizer=\"adam\",\n",
    "                   metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bed11b-e643-4292-b109-468825814b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    982/Unknown \u001b[1m93s\u001b[0m 89ms/step - accuracy: 0.7948 - loss: 0.4066"
     ]
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs= NB_EPOCHS)\n",
    "save_model(text_model, \"sentiment_model_retrain.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cebda7-f8f5-4cca-88f7-cb06c29e27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 58ms/step - accuracy: 0.8953 - loss: 0.4888\n",
      "[0.43716928362846375, 0.8986378312110901]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14f04c-bd1a-4057-a6c2-2e0f7b75d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Avis positif ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "def predict_review(model, review_text):\n",
    "    \n",
    "    cleaned_review = preflitrage_txt(review_text)\n",
    "\n",
    "    \n",
    "    tokenized_review = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(cleaned_review))\n",
    "    review_tensor = tf.convert_to_tensor([tokenized_review])\n",
    "    prediction = model.predict(review_tensor)\n",
    "\n",
    "    \n",
    "    if prediction >= 0.5:\n",
    "        return \"Avis positif ðŸ˜Š\"\n",
    "    else:\n",
    "        return \"Avis nÃ©gatif ðŸ˜ž\"\n",
    "\n",
    "\n",
    "test_review = \"I got bricked up from that movie\"\n",
    "print(predict_review(text_model, test_review))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2daf4-8e47-4c4d-83bf-a061a7a64695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_kernel",
   "language": "python",
   "name": "tensorflow_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
